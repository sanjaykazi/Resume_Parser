{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanjay Kazi\n",
      "\n",
      "sanjaykazi1149@gmail.com\n",
      "\n",
      "+91 7295038827\n",
      "\n",
      "Mumbai, Chennai\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Bachelor of Technology (B.Tech), Metalurgical Engineering And Materiral\n",
      "Science\n",
      "\n",
      "Indian Institute Of Technology Bombay\n",
      "\n",
      "2018 - 2022\n",
      "\n",
      "Senior Secondary (XII), Science\n",
      "\n",
      "Sri Chaitanya Jr. Kalasala Hyderabad\n",
      "\n",
      "(TSBIE board)\n",
      "\n",
      "Year of completion: 2018\n",
      "\n",
      "Percentage: 97.90%\n",
      "\n",
      "Secondary (X)\n",
      "\n",
      "(BSEB board)\n",
      "\n",
      "Year of completion: 2016\n",
      "\n",
      "Percentage: 94.60%\n",
      "\n",
      "Simultala Awasiya Vidyalya Jamui\n",
      "\n",
      "INTERNSHIPS\n",
      "\n",
      "Data Science And Business Analyst\n",
      "\n",
      "The Spark Foundation, Virtual\n",
      "\n",
      "May 2021 - May 2021\n",
      "\n",
      "POSITIONS OF\n",
      "RESPONSIBILITY\n",
      "\n",
      "- Created a hybrid model for stock price/performance prediction using numerical\n",
      "analysis of historical stock prices, and sentimental analysis of news headlines.\n",
      "- Performed EDA on dataset ‘Indian Premier League’ using PowerBI & created\n",
      "storyboard.\n",
      "\n",
      "Campaigns Head |Abhyuday, IIT Bombay [Apr’20 - Apr'21]\n",
      "• Led a 2 tier team of 100+ volunteers; 50+ events & campaigns; 5 nationwide\n",
      "competitions.\n",
      "•Led alliance with ARCH Foundation for initiative iLearn, webinars for 750+\n",
      "students.\n",
      "\n",
      "Hospitality & PR Coordinator | Mood Indigo [May’19 - Dec’19]\n",
      "• Worked in a team of 50 members to manage hospitality demands of over 146k+\n",
      "visitors, artists.\n",
      "• Appointed and coordinated with 30 CRs from colleges all over the country.\n",
      "\n",
      "Social Secretary hostel-2, IIT Bombay\n",
      "• Orchestrated Hostel Fest Fun2shh having a budget of INR 100k and other\n",
      "events throughout tenure.\n",
      "• Supervised Lukkha weekend a two-day event as overall coordinator comprised\n",
      "of various events and competitions\n",
      "\n",
      "TRAININGS\n",
      "\n",
      "AWS Cloud Practitioner Essentials\n",
      "\n",
      "AWS Training And Certi\u0000cation, Online\n",
      "\n",
      "Apr 2021 - Apr 2021\n",
      "\n",
      "\f",
      "PROJECTS\n",
      "\n",
      "Interactive Web App with Streamlit and Scikit-learn\n",
      "\n",
      "Learned the fundamentals of AWS cloud concepts, core AWS services, security,\n",
      "architecture, pricing, and support to build your AWS Cloud knowledge.\n",
      "\n",
      "Credentials:\n",
      "https://www.aws.training/Transcript/CompletionCerti\u0000cateHtml?\n",
      "transcriptid=gCStNUysCUWnMl_W0RUzyA2   \n",
      "\n",
      "Deep Learning A-Z™: Hands-On Arti\u0000cial Neural Networks\n",
      "\n",
      "udemy, Online\n",
      "\n",
      "Feb 2021 - Mar 2021\n",
      "\n",
      "udemy, Online\n",
      "\n",
      "Nov 2020 - Feb 2021\n",
      "\n",
      "Learned to create Deep Learning Algorithms in Python. This course covered\n",
      "supervised and unsupervised deep learning models like ANN, CNN, RNN, SOMs,\n",
      "and Boltzmann Machine\n",
      "\n",
      "Machine Learning A-Z: Hands-On Python & R In Data Science\n",
      "\n",
      "I enjoyed learning Data science and this is very much exciting career \u0000eld for me. I\n",
      "was able to make calculated predictions just having data in my hand and this\n",
      "fascinates me so much. Looking forward to pursue my career in this area.\n",
      "\n",
      "Apr 2021 - Apr 2021\n",
      "\n",
      "https://github.com/sanjaykazi/Streamlit-Project\n",
      "\n",
      "• This interactive web application built using streamlit will allows Exploring\n",
      "different datasets and classi\u0000ers.\n",
      "\n",
      "Object Recognition | CNN\n",
      "\n",
      "Mar 2021 - Mar 2021\n",
      "\n",
      "https://github.com/sanjaykazi/Object-Recognition\n",
      "\n",
      "•This model is built using Keras, a high-level neural network application\n",
      "programming interface (API) that supports both Theano and Tensor\u0000ow\n",
      "backends.\n",
      "\n",
      "Diabetes Prediction | kaggle | Machine Learning\n",
      "\n",
      "Nov 2020 - Nov 2020\n",
      "\n",
      "https://github.com/sanjaykazi/Diabetes_Prediction\n",
      "\n",
      "• Pre-processed the data, applied EDA, and built a classi\u0000er using Random Forest\n",
      "Algorithm. Achieved an accuracy of 94% on the training set and 74.1% on the test\n",
      "set.\n",
      "• Improved accuracy by 7% by the tunning model using RandomizedSearchCV &\n",
      "xgBoost\n",
      "\n",
      "Fake News Analysis | Natural Language Processing\n",
      "\n",
      "Oct 2020 - Oct 2020\n",
      "\n",
      "https://github.com/sanjaykazi/Fake-News-Analysis\n",
      "\n",
      "• Pre-processed data using Vectorization, Tokenization, Lemmatization, and\n",
      "Padding, Developed model using Multinomial Classi\u0000er and passive-aggressive\n",
      "classi\u0000er algorithm. Achieved 92.4% and 90% accuracy on train and test sets.\n",
      "\n",
      "Predicting the CYS of HEA using ML\n",
      "\n",
      "Jan 2021 - Present\n",
      "\n",
      "\f",
      "This is a project being done under my professor, MEMS IIT Bombay.\n",
      "\n",
      "Courses Completed\n",
      "\n",
      "Aug 2018 - Present\n",
      "\n",
      "• Mathematics\n",
      "Calculus, Differential Equation, Linear Algebra, Data Analysis and Interpretation,\n",
      "Introduction to Numerical Analysis\n",
      "• Computer Science\n",
      "Computer Programming and Utilization, Computation Lab[Matlab, Python]\n",
      "\n",
      "SKILLS\n",
      "\n",
      "English Pro\u0000ciency (Spoken)\n",
      "\n",
      "C++ Programming\n",
      "\n",
      "Intermediate\n",
      "\n",
      "Python\n",
      "\n",
      "Intermediate\n",
      "\n",
      "MS-Excel\n",
      "\n",
      "Intermediate\n",
      "\n",
      "SQL\n",
      "\n",
      "Beginner\n",
      "\n",
      "AutoCAD\n",
      "\n",
      "Intermediate\n",
      "\n",
      "HTML\n",
      "\n",
      "Intermediate\n",
      "\n",
      "Bootstrap\n",
      "\n",
      "Intermediate\n",
      "\n",
      "Data Structures\n",
      "\n",
      "Intermediate\n",
      "\n",
      "Intermediate\n",
      "\n",
      "R Programming\n",
      "\n",
      "Intermediate\n",
      "\n",
      "Linux\n",
      "\n",
      "Beginner\n",
      "\n",
      "MATLAB\n",
      "\n",
      "Intermediate\n",
      "\n",
      "Intermediate\n",
      "\n",
      "CSS\n",
      "\n",
      "Intermediate\n",
      "\n",
      "Data Analytics\n",
      "\n",
      "Intermediate\n",
      "\n",
      "JavaScript\n",
      "\n",
      "Beginner\n",
      "\n",
      "Microsoft Visual Studio\n",
      "\n",
      "WORK SAMPLES\n",
      "\n",
      "GitHub pro\u0000le\n",
      "\n",
      "https://github.com/sanjaykazi\n",
      "\n",
      "Other portfolio link\n",
      "\n",
      "https://www.hackerrank.com/sanjaykazi1149\n",
      "\n",
      "ADDITIONAL DETAILS\n",
      "\n",
      "• Participated in volleyball General Championship and won Gold for hostel-2, IIT\n",
      "Bombay. [2020)]\n",
      "• Completed yearlong Water-Polo training under National Sports Organization, IIT\n",
      "Bombay[2019]\n",
      "\n",
      "• Was captain of 3 times champion in a row Volleyball team in SAV [2013, ’14, ’15]\n",
      "• Secured 1st position in Relay race in SAV organized on National Sports Day\n",
      "[2015]\n",
      "\n",
      "• Secured 2nd position out of 20+ entries in Freshmen Short Film Making\n",
      "competition [2018]\n",
      "• Participated in Conventional Processing and Shaping of Oxide Ceramics at Ants\n",
      "Ceramics [2019]\n",
      "\n",
      "\f",
      "• Completed Consult, Finance, Analytic, Entrepreneurship, Management and\n",
      "Python boot camps in\n",
      "summer school [Apr’19 - Jun’19]\n",
      "\n",
      "• Recipient of Dr. Ambedkar National Merit Award as a certi\u0000cate and 60K\n",
      "amount [2016]\n",
      "• Recipient of prestigious National Talent Search Examination scholarship\n",
      "organised by NCERT [2016]\n",
      "\n",
      "• Secured 9th rank in Bihar School Examination Board BSEB out of 1,547,000\n",
      "candidates [2016]\n",
      "• Ranked 46th in International Talent Hunt Olympiad organised by Silver Zone\n",
      "Foundation [2014]\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(extract_text_from_pdf('san_cv.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    text = docx2txt.process(docx_path)\n",
    "    if text:\n",
    "        return text.replace('\\t' ' ')\n",
    "    return None\n",
    "\n",
    "if __name__ == '__mian__':\n",
    "    print(extract_text_from_docx('resume_001.docx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = docx2txt.process('resume_001.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+1 (970) 333-3833  \n",
      "\n",
      "matthew.eliot@mail.com \n",
      "\n",
      "https://linkedin.com/mattheweliot\n",
      "\n",
      "+1 (970) 333-3833  \n",
      "\n",
      "matthew.eliot@mail.com \n",
      "\n",
      "https://linkedin.com/mattheweliot\n",
      "\n",
      "MATTHEW ELIOT\n",
      "\n",
      "MATTHEW ELIOT\n",
      "\n",
      "Summary\n",
      "\n",
      "Senior Web Developer specializing in front end development. Experienced with all stages of the development cycle for dynamic web projects. Well-versed in numerous programming languages including HTML5, PHP OOP, JavaScript, CSS, MySQL. Strong background in project management and customer relations.\n",
      "\n",
      "Skill Highlights\n",
      "\n",
      "Project management\n",
      "\n",
      "Strong decision maker\n",
      "\n",
      "Complex problem solver\n",
      "\n",
      "Creative design\n",
      "\n",
      "Innovative\n",
      "\n",
      "Service-focused\n",
      "\n",
      "Experience\n",
      "\n",
      "Web Developer - 09/2015 to 05/2019\n",
      "\n",
      "Luna Web Design, New York\n",
      "\n",
      "Cooperate with designers to create clean interfaces and simple, intuitive interactions and experiences.\n",
      "\n",
      "Develop project concepts and maintain optimal workflow.\n",
      "\n",
      "Work with senior developer to manage large, complex design projects for corporate clients.\n",
      "\n",
      "Complete detailed programming and development tasks for front end public and internal websites as well as challenging back-end server code.\n",
      "\n",
      "Carry out quality assurance tests to discover errors and optimize usability.\n",
      "\n",
      "Education\n",
      "\n",
      "Bachelor of Science: Computer Information Systems - 2014\n",
      "\n",
      "Columbia University, NY\n",
      "\n",
      "Certifications\n",
      "\n",
      "PHP Framework (certificate): Zend, Codeigniter, Symfony.\n",
      "\n",
      "Programming Languages: JavaScript, HTML5, PHP OOP, CSS, SQL, MySQL.\n",
      "\n",
      "\n",
      "\n",
      "Summary\n",
      "\n",
      "Senior Web Developer specializing in front end development. Experienced with all stages of the development cycle for dynamic web projects. Well-versed in numerous programming languages including HTML5, PHP OOP, JavaScript, CSS, MySQL. Strong background in project management and customer relations.\n",
      "\n",
      "Skill Highlights\n",
      "\n",
      "Project management\n",
      "\n",
      "Strong decision maker\n",
      "\n",
      "Complex problem solver\n",
      "\n",
      "Creative design\n",
      "\n",
      "Innovative\n",
      "\n",
      "Service-focused\n",
      "\n",
      "Experience\n",
      "\n",
      "Web Developer - 09/2015 to 05/2019\n",
      "\n",
      "Luna Web Design, New York\n",
      "\n",
      "Cooperate with designers to create clean interfaces and simple, intuitive interactions and experiences.\n",
      "\n",
      "Develop project concepts and maintain optimal workflow.\n",
      "\n",
      "Work with senior developer to manage large, complex design projects for corporate clients.\n",
      "\n",
      "Complete detailed programming and development tasks for front end public and internal websites as well as challenging back-end server code.\n",
      "\n",
      "Carry out quality assurance tests to discover errors and optimize usability.\n",
      "\n",
      "Education\n",
      "\n",
      "Bachelor of Science: Computer Information Systems - 2014\n",
      "\n",
      "Columbia University, NY\n",
      "\n",
      "Certifications\n",
      "\n",
      "PHP Framework (certificate): Zend, Codeigniter, Symfony.\n",
      "\n",
      "Programming Languages: JavaScript, HTML5, PHP OOP, CSS, SQL, MySQL.\n",
      "\n",
      "Natasha jain\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sanjay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sanjay/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/sanjay/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/sanjay/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanjay\n"
     ]
    }
   ],
   "source": [
    "# example_04.py\n",
    "\n",
    "import docx2txt\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_names(txt):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(txt):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    "\n",
    "    return person_names\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('san_cv.pdf')\n",
    "    names = extract_names(text)\n",
    "\n",
    "    if names:\n",
    "        print(names[0])  # noqa: T001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Phone Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+91 7295038827\n"
     ]
    }
   ],
   "source": [
    "# example_05.py\n",
    "\n",
    "import re\n",
    "import subprocess  # noqa: S404\n",
    "\n",
    "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "\n",
    "\n",
    "def doc_to_text_catdoc(file_path):\n",
    "    try:\n",
    "        process = subprocess.Popen(  # noqa: S607,S603\n",
    "            ['catdoc', '-w', file_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "    except (\n",
    "        FileNotFoundError,\n",
    "        ValueError,\n",
    "        subprocess.TimeoutExpired,\n",
    "        subprocess.SubprocessError,\n",
    "    ) as err:\n",
    "        return (None, str(err))\n",
    "    else:\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "    return (stdout.strip(), stderr.strip())\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_phone_number(resume_text):\n",
    "    phone = re.findall(PHONE_REG, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('san_cv.pdf')\n",
    "    phone_number = extract_phone_number(text)\n",
    "\n",
    "    print(phone_number)  # noqa: T001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanjaykazi1149@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# example_06.py\n",
    "\n",
    "import re\n",
    "\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "\n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('san_cv.pdf')\n",
    "    emails = extract_emails(text)\n",
    "\n",
    "    if emails:\n",
    "        print(emails[0])  # noqa: T001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXtracting skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sanjay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Python', 'Machine Learning', 'Data Science', 'Data science'}\n"
     ]
    }
   ],
   "source": [
    "# example_07.py\n",
    "\n",
    "import docx2txt\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# you may read the database from a csv file or some other database\n",
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "]\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "\n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "\n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    "\n",
    "    return found_skills\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('san_cv.pdf')\n",
    "    skills = extract_skills(text)\n",
    "\n",
    "    print(skills)  # noqa: T001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skills using skills API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sanjay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Invalid authentication credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1c249036e100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_from_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'san_cv.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mskills\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_skills\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskills\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# noqa: T001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1c249036e100>\u001b[0m in \u001b[0;36mextract_skills\u001b[0;34m(input_text)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# we search for each token in our skills database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiltered_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mskill_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mfound_skills\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-1c249036e100>\u001b[0m in \u001b[0;36mskill_exists\u001b[0;34m(skill)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mskill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Invalid authentication credentials"
     ]
    }
   ],
   "source": [
    "# example_08.py\n",
    "\n",
    "import docx2txt\n",
    "import nltk\n",
    "import requests\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "\n",
    "def skill_exists(skill):\n",
    "    url = f'https://api.promptapi.com/skills?q={skill}&count=1'\n",
    "    headers = {'apikey': 'YOUR API KEY'}\n",
    "    response = requests.request('GET', url, headers=headers)\n",
    "    result = response.json()\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return len(result) > 0 and result[0].lower() == skill.lower()\n",
    "    raise Exception(result.get('message'))\n",
    "    \n",
    "    pass\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    "\n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "\n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    "\n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "\n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    "\n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if skill_exists(token.lower()):\n",
    "            found_skills.add(token)\n",
    "\n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if skill_exists(ngram.lower()):\n",
    "            found_skills.add(ngram)\n",
    "\n",
    "    return found_skills\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('san_cv.pdf')\n",
    "    skills = extract_skills(text)\n",
    "\n",
    "    print(skills)  # noqa: T001\n",
    "    \n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School and college"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_09.py\n",
    "\n",
    "import docx2txt\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "\n",
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'univers',\n",
    "    'academy',\n",
    "    'faculty',\n",
    "    'institute',\n",
    "    'faculdades',\n",
    "    'Schola',\n",
    "    'schule',\n",
    "    'lise',\n",
    "    'lyceum',\n",
    "    'lycee',\n",
    "    'polytechnic',\n",
    "    'kolej',\n",
    "    'ünivers',\n",
    "    'okul',\n",
    "]\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_education(input_text):\n",
    "    organizations = []\n",
    "\n",
    "    # first get all the organization names using nltk\n",
    "    for sent in nltk.sent_tokenize(input_text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "\n",
    "    # we search for each bigram and trigram for reserved words\n",
    "    # (college, university etc...)\n",
    "    education = set()\n",
    "    for org in organizations:\n",
    "        for word in RESERVED_WORDS:\n",
    "            if org.lower().find(word) >= 0:\n",
    "                education.add(org)\n",
    "\n",
    "    return education\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('san_cv.pdf')\n",
    "    education_information = extract_education(text)\n",
    "\n",
    "    print(education_information)  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S','C.A.','c.a.','B.Com','B. Com','M. Com', 'M.Com','M. Com .',\n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S',\n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH',\n",
    "            'PHD', 'phd', 'ph.d', 'Ph.D.','MBA','mba','graduate', 'post-graduate','5 year integrated masters','masters',\n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_education(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        #print(index, text), print('-'*50)\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            print(tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "                print(edu.keys())\n",
    "text = extract_text_from_pdf('san_cv.pdf')\n",
    "print(extract_education(text)) #resume parsed into text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "\n",
    "def extract_education1(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "text = extract_text_from_pdf('san_cv.pdf')\n",
    "extract_education1(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Grad all general stop words\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# Education Degrees\n",
    "EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "        ]\n",
    "\n",
    "def extract_education1(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # Sentence Tokenizer\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "\n",
    "    edu = {}\n",
    "    # Extract education degree\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            # Replace all special symbols\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in STOPWORDS:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "\n",
    "    # Extract year\n",
    "    education = []\n",
    "    for key in edu.keys():\n",
    "        year = re.search(re.compile(r'(((20|19)(\\d{2})))'), edu[key])\n",
    "        if year:\n",
    "            education.append((key, ''.join(year[0])))\n",
    "        else:\n",
    "            education.append(key)\n",
    "    return education\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_09.py\n",
    "\n",
    "import docx2txt\n",
    "import nltk\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "\n",
    "RESERVED_WORDS = [\n",
    "    'BE','B.E.', 'B.E', 'BS', 'B.S', \n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S', \n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH', \n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "]\n",
    "\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    txt = docx2txt.process(docx_path)\n",
    "    if txt:\n",
    "        return txt.replace('\\t', ' ')\n",
    "    return None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_education(input_text):\n",
    "    organizations = []\n",
    "\n",
    "    # first get all the organization names using nltk\n",
    "    for sent in nltk.sent_tokenize(input_text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "\n",
    "    # we search for each bigram and trigram for reserved words\n",
    "    # (college, university etc...)\n",
    "    education = set()\n",
    "    for org in organizations:\n",
    "        for word in RESERVED_WORDS:\n",
    "            if org.upper().find(word) >= 0:\n",
    "                education.add(org)\n",
    "\n",
    "    return education\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('san_cv.pdf')\n",
    "    education_information = extract_education(text)\n",
    "\n",
    "    print(education_information)  # noqa: T001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
